FAQ for Week 7 and programming exercise 6
MentorWeek 7 · 3 months ago · Edited

VIDEO LECTURE FAQ

Q1) How are the landmarks defined and used?

For each training example, the landmarks are all of the other training examples.

The SVM method is based on measuring the similarity between all pairs of training examples, then selecting a subset of them that will define the support vectors - that is, the vectors which provide the largest margin available for the given constraints.

Q2) Why do Prof Ng's lectures about SVM use theta and f() and cost(i), but these don't appear in the programming exercise?

Prof Ng's lectures provide an intuition about how the SVM works. But this isn't a practical method of finding a solution. The programming exercises uses the SMO (sequential minimial optimization) method, which is provided for you in the exercise scripts as the "svmTrain()" and "svmPredict()" functions. These use a data structure called the "model" that represents the factors used in the SMO method.

More information about the SMO method is available in the Course Wiki programming exercise notes for ex6.

Q3) How do we know that the theta is perpendicular to the decision boundary?

(thanks to Mentor Chirag for this derivation)

We know that θ′∗x=0 for any x on the boundary since the boundary is where sigmoid = 0.5.

11+e−z=12=>e−z=1=>−z=0=>θ′∗x=0

So pick two random points on the boundary a and b, then

θ′∗a=0 and θ′∗b=0

=> θ′∗(a−b)=0 => θ′⋅(a−b)=0

and we know that when the dot product of two vectors is 0, the angle between them is 90 degrees. And since the vector (a-b) is on the decision boundary, θ is perpendicular to the decision boundary.

Q4) How do I compute the answer to the quiz in the video "Mathematics Behind Large Margin Classification"?

The key is this bit from the lecture slides:

p(i) is the projection of x onto the theta axis. From the image in the video, you can see that equals 2. Plug this into the equation inthe blue box and solve for the norm of θ

QUIZ FAQ

PROGRAMMING ASSIGNMENT FAQ

Q1) What does this line of code do (from ex6.m at line 108)?

model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma));

This is the best line of code to use as a guide for the dataset3Params() function.

It uses the "anonymous function" method.

'x1' and 'x2' are dummy parameters. They are filled-in at runtime when svmTrain() calls your kernel function.

'x1' and 'x2 represent a pair of training examples. svmTrain() computes the similarity between all pairs of training examples.

The values you pass for 'C' and 'sigma' must be scalars.

svmTrain() returns a data structure 'model', which specifies the parameters of the SMO (sequential minimal optimization) algorithm for computing the SVM. You can learn more about the SMO method in the Course Wiki - programming exercise notes.

Q2a) Why is the decision boundary not drawn?

...or...

Q2b) Why do I get this error message?

"error: set: unknown hggroup property Color"

There is an error in the visualizeBoundary.m script. The fix is discussed in the Tutorial for ex6 - see the Resources menu for the list of tutorials.
